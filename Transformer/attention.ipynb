{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2491bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51f3af31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.8519, 0.4641, 0.4086,  ..., 0.3193, 0.8218, 0.6690],\n",
      "         [0.9595, 0.3154, 0.9714,  ..., 0.7730, 0.4269, 0.5733],\n",
      "         [0.1641, 0.4766, 0.5172,  ..., 0.9114, 0.1794, 0.5730],\n",
      "         ...,\n",
      "         [0.0469, 0.7299, 0.6926,  ..., 0.9576, 0.9861, 0.3480],\n",
      "         [0.6076, 0.6442, 0.8933,  ..., 0.2551, 0.0993, 0.9793],\n",
      "         [0.5506, 0.9161, 0.0407,  ..., 0.0694, 0.5707, 0.8420]],\n",
      "\n",
      "        [[0.4210, 0.5858, 0.7163,  ..., 0.3907, 0.2523, 0.8049],\n",
      "         [0.6762, 0.4809, 0.4451,  ..., 0.8268, 0.9438, 0.4585],\n",
      "         [0.1543, 0.0526, 0.7058,  ..., 0.7519, 0.9232, 0.6365],\n",
      "         ...,\n",
      "         [0.5103, 0.2334, 0.8384,  ..., 0.0545, 0.9776, 0.6434],\n",
      "         [0.8146, 0.4679, 0.0789,  ..., 0.6532, 0.3450, 0.6298],\n",
      "         [0.0552, 0.6068, 0.3791,  ..., 0.7305, 0.2098, 0.4063]],\n",
      "\n",
      "        [[0.9523, 0.3587, 0.1386,  ..., 0.3348, 0.5680, 0.0610],\n",
      "         [0.9646, 0.0879, 0.4080,  ..., 0.1971, 0.0317, 0.2476],\n",
      "         [0.8919, 0.5665, 0.7947,  ..., 0.4214, 0.0965, 0.3594],\n",
      "         ...,\n",
      "         [0.5176, 0.0336, 0.0948,  ..., 0.1961, 0.5173, 0.3652],\n",
      "         [0.3545, 0.6163, 0.5956,  ..., 0.8361, 0.1721, 0.3127],\n",
      "         [0.6943, 0.7253, 0.0218,  ..., 0.4362, 0.4858, 0.2685]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.3481, 0.1638, 0.3941,  ..., 0.8284, 0.9212, 0.3116],\n",
      "         [0.2471, 0.3021, 0.4592,  ..., 0.9128, 0.3141, 0.2907],\n",
      "         [0.3448, 0.4201, 0.4495,  ..., 0.0712, 0.4999, 0.5889],\n",
      "         ...,\n",
      "         [0.3850, 0.4796, 0.5416,  ..., 0.8373, 0.1496, 0.7937],\n",
      "         [0.3626, 0.9537, 0.3284,  ..., 0.4257, 0.2129, 0.9204],\n",
      "         [0.4615, 0.5255, 0.9075,  ..., 0.0067, 0.2631, 0.2621]],\n",
      "\n",
      "        [[0.5129, 0.7867, 0.3470,  ..., 0.0579, 0.6903, 0.8675],\n",
      "         [0.9586, 0.2704, 0.0535,  ..., 0.6061, 0.5874, 0.0586],\n",
      "         [0.7964, 0.1360, 0.0772,  ..., 0.6326, 0.3371, 0.5153],\n",
      "         ...,\n",
      "         [0.7279, 0.9735, 0.2486,  ..., 0.9972, 0.2136, 0.7003],\n",
      "         [0.7772, 0.8353, 0.2107,  ..., 0.3295, 0.9454, 0.4499],\n",
      "         [0.9414, 0.2599, 0.8812,  ..., 0.9872, 0.8778, 0.2047]],\n",
      "\n",
      "        [[0.0565, 0.8561, 0.5852,  ..., 0.1321, 0.3724, 0.2747],\n",
      "         [0.9049, 0.7608, 0.4060,  ..., 0.3669, 0.0074, 0.6602],\n",
      "         [0.6429, 0.9434, 0.5995,  ..., 0.9984, 0.9887, 0.6048],\n",
      "         ...,\n",
      "         [0.6712, 0.3767, 0.3408,  ..., 0.8289, 0.2068, 0.9650],\n",
      "         [0.8762, 0.8580, 0.3187,  ..., 0.0551, 0.2157, 0.6463],\n",
      "         [0.5150, 0.6149, 0.4757,  ..., 0.2388, 0.7686, 0.9308]]])\n"
     ]
    }
   ],
   "source": [
    "x=torch.rand(128,32,512)\n",
    "d_model=512\n",
    "n_head=8\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eabc7ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MutiHeadAttention(nn.Module):\n",
    "    def __init__(self,d_model,n_head):\n",
    "        super(MutiHeadAttention,self).__init__()\n",
    "        self.n_head=n_head\n",
    "        #注意力头数\n",
    "        self.d_model=d_model\n",
    "        #输入输出特征维度\n",
    "        ##线性映射层，将输入映射为 Q K W\n",
    "        self.w_q=nn.Linear(d_model,n_head)\n",
    "        #     Args:\n",
    "        # in_features: size of each input sample\n",
    "        # out_features: size of each output sample\n",
    "        # bias: If set to ``False``, the layer will not learn an additive bias.\n",
    "        #     Default: ``True``\n",
    "        #y=xW+b 数学的线性变换  \n",
    "        self.w_k=nn.Linear(d_model,n_head)\n",
    "        self.w_v=nn.Linear(d_model,n_head)\n",
    "        #输出融合层，将多头输出并合并会d_model\n",
    "        self.w_combine=nn.Linear(d_model,n_head)\n",
    "        self.softmax=nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self,q,k,v,mask=None):\n",
    "        batch,time,dimension=q.shape       #batch   序列长度    特征维度\n",
    "        n_d=self.d_model//self.n_head      #每个头的维度=特征维度/头的个数\n",
    "        #\n",
    "        q,k,v=self.w_q(q),self.w_k(k),self.w_v(v)   #q,k,v线性映射成Q,K,V\n",
    "        q=q.view(batch,time,self.n_head,n_d).permute(0,2,1,3)\n",
    "        #view从后往前看，把tensor先变成(sum,)，然后从最后一个维度开始分块，依次往前\n",
    "        #permute就是维度互换    从左往后对齐序列\n",
    "        k=k.view(batch,time,self.n_head,n_d).permute(0,2,1,3)\n",
    "        v=v.view(batch,time,self.n_head,n_d).permute(0,2,1,3)\n",
    "        score=q@k.transpose(2,3)/math.sqrt(n_d)\n",
    "        #Q维度batch,time,self.n_head,n_d        K^T维度batch,time,n_d,self.n_head\n",
    "        #transpose 单独两个维度进行互换\n",
    "        #高纬tensor相乘规则：矩阵的最后两维满足矩阵相乘原则，前面的n-2维相同（或者利用广播机制 可以为1）\n",
    "        #score batch,time,n_head,n_head\n",
    "        if mask is not None:\n",
    "            score=score.masked_fill(mask==0,-100000)#将无效位置填充为-100000 mask==0就是前面提到的padding=0\n",
    "            score=self.softmax(score)@v\n",
    "            score=score.permute(0,2,1,3).contiguous().view(batch,time,dimension)\n",
    "            #score batch,time,n_head,n_head       \n",
    "            output=self.w_combine(score)\n",
    "            #self定义的self.w_combine=nn.Linear(d_model,n_head)         融合多头注意力\n",
    "            return output\n",
    "\n",
    "\n",
    "attention=MutiHeadAttention(d_model,n_head)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ae805d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/sunqingxin/anaconda3/envs/transformer/lib/python3.10/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_vars.py\", line 622, in change_attr_expression\n",
      "    value = eval(expression, frame.f_globals, frame.f_locals)\n",
      "  File \"<string>\", line 1, in <module>\n",
      "NameError: name 'tensor' is not defined\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[128, 32, 8, 64]' is invalid for input of size 32768",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out\u001b[38;5;241m=\u001b[39m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(out)\n",
      "File \u001b[0;32m~/anaconda3/envs/transformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/transformer/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[3], line 27\u001b[0m, in \u001b[0;36mMutiHeadAttention.forward\u001b[0;34m(self, q, k, v, mask)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m     26\u001b[0m q,k,v\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw_q(q),\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw_k(k),\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw_v(v)   \u001b[38;5;66;03m#q,k,v线性映射成Q,K,V\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m q\u001b[38;5;241m=\u001b[39m\u001b[43mq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtime\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_head\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_d\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#view从后往前看，把tensor先变成(sum,)，然后从最后一个维度开始分块，依次往前\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#permute就是维度互换    从左往后对齐序列\u001b[39;00m\n\u001b[1;32m     30\u001b[0m k\u001b[38;5;241m=\u001b[39mk\u001b[38;5;241m.\u001b[39mview(batch,time,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_head,n_d)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[128, 32, 8, 64]' is invalid for input of size 32768"
     ]
    }
   ],
   "source": [
    "out=attention(x,x,x)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a269cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f90122",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c82954",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
